# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QATqse9a03Q8h4C2ywMxq2yUlA7qyOR8
"""

!cp /content/drive/MyDrive/new_test.zip /content/
!cp /content/drive/MyDrive/new_train.zip /content/
!mkdir /content/new_train
!mkdir /content/new_test
!unzip -q /content/new_train.zip -d /content/new_train
!unzip -q /content/new_test.zip -d /content/new_test

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader
import os

train_dir = '/content/new_train'
test_dir = '/content/new_test'

# Sadece normalize et, augment yok
mean = [0.485, 0.456, 0.406]
std = [0.229, 0.224, 0.225]
image_size = 224

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomRotation(degrees=20),
    transforms.RandomResizedCrop(size=224, scale=(0.85, 1.0)),  # el pozisyonu çeşitliliği
    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),
    transforms.RandomHorizontalFlip(p=0.5),  # bazı işaretler simetrik değil, dikkat!
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])
test_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

train_dataset = datasets.ImageFolder(train_dir, transform=transform)
test_dataset = datasets.ImageFolder(test_dir, transform=test_transform)
print(train_dataset.class_to_idx)
print(test_dataset.class_to_idx)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

num_classes = len(train_dataset.classes)
print("Sınıf sayısı:", num_classes)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- 1. GoogLeNet ---
googlenet = models.googlenet(pretrained=True)
for name, param in googlenet.named_parameters():
    if 'inception5' in name or 'fc' in name:
        param.requires_grad = True
    else:
        param.requires_grad = False

googlenet.fc = nn.Linear(1024, num_classes)  # Son katmanı değiştir
googlenet = googlenet.to(device)

# --- 2. ResNet18 ---
resnet18 = models.resnet18(pretrained=True)
for name, param in resnet18.named_parameters():
    if 'layer4' in name or 'fc' in name:
        param.requires_grad = True
    else:
        param.requires_grad = False

resnet18.fc = nn.Linear(512, num_classes)
resnet18 = resnet18.to(device)

# --- 3. DenseNet121 ---
densenet121 = models.densenet121(pretrained=True)
for name, param in densenet121.named_parameters():
    if 'denseblock4' in name or 'classifier' in name:
        param.requires_grad = True
    else:
        param.requires_grad = False

densenet121.classifier = nn.Linear(1024, num_classes)
densenet121 = densenet121.to(device)

# Parametreleri doğrulama
print("GoogLeNet eğitilebilir parametre sayısı:", sum(p.numel() for p in googlenet.parameters() if p.requires_grad))
print("ResNet18 eğitilebilir parametre sayısı:", sum(p.numel() for p in resnet18.parameters() if p.requires_grad))
print("DenseNet121 eğitilebilir parametre sayısı:", sum(p.numel() for p in densenet121.parameters() if p.requires_grad))

criterion = nn.CrossEntropyLoss()

from collections import Counter
targets = train_dataset.targets  # doğrudan etiket listesi, çok hızlı
class_counts = Counter(targets)
counts = torch.tensor([class_counts[i] for i in range(num_classes)], dtype=torch.float)
weights = 1.0 / counts
weights = weights / weights.sum()  # normalize (şart değil ama önerilir)
weights = weights.to(device)

criterion = nn.CrossEntropyLoss(weight=weights)

# Sadece son katman parametrelerini optimize ediyoruz
optimizer = optim.Adam(
    list(filter(lambda p: p.requires_grad, googlenet.parameters())) +
    list(filter(lambda p: p.requires_grad, resnet18.parameters())) +
    list(filter(lambda p: p.requires_grad, densenet121.parameters())),
    lr=1e-4  # Daha düşük öğrenme oranı genellikle daha iyidir transfer learning için
)


from tqdm import tqdm

def train_epoch(models, dataloader, optimizer, criterion, device):
    googlenet, resnet18, densenet121 = models
    googlenet.train()
    resnet18.train()
    densenet121.train()

    total_loss = 0
    total_correct = 0
    total_samples = 0

    loop = tqdm(dataloader, desc="Training", leave=False)

    for images, labels in loop:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()

        out1 = googlenet(images)
        out2 = resnet18(images)
        out3 = densenet121(images)

        outputs = (out1 + out2 + out3) / 3.0

        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * images.size(0)
        _, preds = torch.max(outputs, 1)
        total_correct += (preds == labels).sum().item()
        total_samples += images.size(0)

        loop.set_postfix(loss=loss.item())

    avg_loss = total_loss / total_samples
    accuracy = total_correct / total_samples
    return avg_loss, accuracy

def evaluate(models, dataloader, criterion, device):
    googlenet, resnet18, densenet121 = models
    googlenet.eval()
    resnet18.eval()
    densenet121.eval()

    total_loss = 0
    total_correct = 0
    total_samples = 0

    loop = tqdm(dataloader, desc="Evaluating", leave=False)

    with torch.no_grad():
        for images, labels in loop:
            images, labels = images.to(device), labels.to(device)

            out1 = googlenet(images)
            out2 = resnet18(images)
            out3 = densenet121(images)

            outputs = (out1 + out2 + out3) / 3.0

            loss = criterion(outputs, labels)
            total_loss += loss.item() * images.size(0)

            _, preds = torch.max(outputs, 1)
            total_correct += (preds == labels).sum().item()
            total_samples += images.size(0)

            loop.set_postfix(loss=loss.item())

    avg_loss = total_loss / total_samples
    accuracy = total_correct / total_samples
    return avg_loss, accuracy

num_epochs = 12  # İstersen artırabiliriz

best_loss = float('inf')
patience = 3  # Gelişme olmadan bekleyeceğimiz epoch sayısı
counter = 0

for epoch in range(num_epochs):
    train_loss, train_acc = train_epoch(
        (googlenet, resnet18, densenet121),
        train_loader,
        optimizer,
        criterion,
        device
    )

    test_loss, test_acc = evaluate(
        (googlenet, resnet18, densenet121),
        test_loader,
        criterion,
        device
    )

    print(f"Epoch {epoch+1}/{num_epochs}")
    print(f"  Train Loss: {train_loss:.4f}, Accuracy: {train_acc*100:.2f}%")
    print(f"  Test  Loss: {test_loss:.4f}, Accuracy: {test_acc*100:.2f}%")

    # --- Early Stopping kontrolü ---
    if test_loss < best_loss:
        best_loss = test_loss
        counter = 0
        print("Test loss azaldı, model kaydediliyor...")
        torch.save({
            'googlenet': googlenet.state_dict(),
            'resnet18': resnet18.state_dict(),
            'densenet121': densenet121.state_dict(),
        }, 'best_model.pth')
    else:
        counter += 1
        print(f"Test loss iyileşmedi. EarlyStopping counter: {counter}/{patience}")
        if counter >= patience:
            print("Erken durdurma tetiklendi.")
            break

!cp /content/best_model.pth /content/drive/MyDrive/last_best_model.pth